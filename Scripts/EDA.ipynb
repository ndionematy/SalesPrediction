{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-family: Times New Roman\">Ecole nationale de la Statistique et de l'Analyse économique - Pierre NDIAYE</p>\n",
    "\n",
    "<p style=\"text-align: center; margin-up: 1000px; font-size: 1rem; font-family: Times New Roman\">\n",
    "<b>Rédigé par :</b><br>\n",
    "Mouhamadi Bassirou COMPAORE<br>\n",
    "Maty NDIONE<br>\n",
    "Elèves Ingénieurs Statisticiens Economistes en 2e année\n",
    "</p>\n",
    "\n",
    "<h1 style=\"color:rgb(1, 100, 120); font-family: Georgia;text-align: center;\">Analyse exploratoire</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-family: Times New Roman;\">L'analyse exploratoire permet d'examiner les données brutes pour en extraire des informations clés pouvant influencer les ventes de Favorita. Elle permet également de comprendre les tendances sous-jacentes, et identifier les anomalies ou les valeurs aberrantes dans les données.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(1, 100, 120); font-family: Georgia;\">Données</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-family: Times New Roman;\">Les données mises à notre disposition sont compressées sous une extension .7zr</p>\n",
    "<p style=\"text-align: justify; font-family: Times New Roman;\">Dans cette partie, elles sont dézipées et extraites pour être enregistré dans le dossier Extrated</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['holidays_events.csv', 'items.csv', 'oil.csv', 'sample_submission.csv', 'stores.csv', 'test.csv', 'train.csv', 'transactions.csv']\n"
     ]
    }
   ],
   "source": [
    "# Chemin contenant les fichiers csv\n",
    "data_path = '..\\Extracted'\n",
    "data_files = os.listdir(data_path)\n",
    "\n",
    "# Données extraites\n",
    "print(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du fichier holidays_events.csv:\n",
      "         date     type    locale locale_name                    description  \\\n",
      "0  2012-03-02  Holiday     Local       Manta             Fundacion de Manta   \n",
      "1  2012-04-01  Holiday  Regional    Cotopaxi  Provincializacion de Cotopaxi   \n",
      "2  2012-04-12  Holiday     Local      Cuenca            Fundacion de Cuenca   \n",
      "3  2012-04-14  Holiday     Local    Libertad      Cantonizacion de Libertad   \n",
      "4  2012-04-21  Holiday     Local    Riobamba      Cantonizacion de Riobamba   \n",
      "\n",
      "   transferred  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n",
      "================================================================================\n",
      "Contenu du fichier items.csv:\n",
      "   item_nbr        family   class  perishable\n",
      "0   96995.0     GROCERY I  1093.0         0.0\n",
      "1   99197.0     GROCERY I  1067.0         0.0\n",
      "2  103501.0      CLEANING  3008.0         0.0\n",
      "3  103520.0     GROCERY I  1028.0         0.0\n",
      "4  103665.0  BREAD/BAKERY  2712.0         1.0\n",
      "================================================================================\n",
      "Contenu du fichier oil.csv:\n",
      "         date  dcoilwtico\n",
      "0  2013-01-01         NaN\n",
      "1  2013-01-02       93.14\n",
      "2  2013-01-03       92.97\n",
      "3  2013-01-04       93.12\n",
      "4  2013-01-07       93.20\n",
      "================================================================================\n",
      "Contenu du fichier sample_submission.csv:\n",
      "            id  unit_sales\n",
      "0  125497040.0         0.0\n",
      "1  125497041.0         0.0\n",
      "2  125497042.0         0.0\n",
      "3  125497043.0         0.0\n",
      "4  125497044.0         0.0\n",
      "================================================================================\n",
      "Contenu du fichier stores.csv:\n",
      "   store_nbr           city                           state type  cluster\n",
      "0        1.0          Quito                       Pichincha    D     13.0\n",
      "1        2.0          Quito                       Pichincha    D     13.0\n",
      "2        3.0          Quito                       Pichincha    D      8.0\n",
      "3        4.0          Quito                       Pichincha    D      9.0\n",
      "4        5.0  Santo Domingo  Santo Domingo de los Tsachilas    D      4.0\n",
      "================================================================================\n",
      "Contenu du fichier test.csv:\n",
      "            id        date  store_nbr  item_nbr onpromotion\n",
      "0  125497040.0  2017-08-16        1.0   96995.0       False\n",
      "1  125497041.0  2017-08-16        1.0   99197.0       False\n",
      "2  125497042.0  2017-08-16        1.0  103501.0       False\n",
      "3  125497043.0  2017-08-16        1.0  103520.0       False\n",
      "4  125497044.0  2017-08-16        1.0  103665.0       False\n",
      "================================================================================\n",
      "Contenu du fichier train.csv:\n",
      "    id        date  store_nbr  item_nbr  unit_sales onpromotion\n",
      "0  0.0  2013-01-01       25.0  103665.0         7.0        <NA>\n",
      "1  1.0  2013-01-01       25.0  105574.0         1.0        <NA>\n",
      "2  2.0  2013-01-01       25.0  105575.0         2.0        <NA>\n",
      "3  3.0  2013-01-01       25.0  108079.0         1.0        <NA>\n",
      "4  4.0  2013-01-01       25.0  108701.0         1.0        <NA>\n",
      "================================================================================\n",
      "Contenu du fichier transactions.csv:\n",
      "         date  store_nbr  transactions\n",
      "0  2013-01-01       25.0         770.0\n",
      "1  2013-01-02        1.0        2111.0\n",
      "2  2013-01-02        2.0        2358.0\n",
      "3  2013-01-02        3.0        3487.0\n",
      "4  2013-01-02        4.0        1922.0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tous les fichiers de données\n",
    "files = [f for f in data_files if f.endswith('.csv')]\n",
    "\n",
    "# Chargement des fichiers csv\n",
    "data = {}\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    try:\n",
    "        df = dd.read_csv(file_path, assume_missing=True)\n",
    "        data[file.replace('.csv', '')] = df\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture de {file}: {e}\")\n",
    "\n",
    "\n",
    "# Afficher les données des fichiers CSV\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    try:\n",
    "        df = dd.read_csv(file_path, dtype={\"onpromotion\": \"object\"}, assume_missing=True)\n",
    "        print(f\"Contenu du fichier {file}:\")\n",
    "        print(df.head())\n",
    "        print(\"=\"*80)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture de {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(1, 100, 120); font-family: Georgia;\">Fusion des données</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "train = data['train']\n",
    "items = data['items']\n",
    "stores = data['stores']\n",
    "holidays_events = data['holidays_events']\n",
    "transactions = data['transactions']\n",
    "oil = data['oil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables du dataframe holidays_events.csv:\n",
      "Nombre d'observations:  350\n",
      "Index(['date', 'type', 'locale', 'locale_name', 'description', 'transferred'], dtype='object')\n",
      "350\n",
      "==================================================\n",
      "Variables du dataframe items.csv:\n",
      "Nombre d'observations:  4100\n",
      "Index(['item_nbr', 'family', 'class', 'perishable'], dtype='object')\n",
      "==================================================\n",
      "Variables du dataframe oil.csv:\n",
      "Nombre d'observations:  1218\n",
      "Index(['date', 'dcoilwtico'], dtype='object')\n",
      "1218\n",
      "==================================================\n",
      "Variables du dataframe sample_submission.csv:\n",
      "Nombre d'observations:  3370464\n",
      "Index(['id', 'unit_sales'], dtype='object')\n",
      "==================================================\n",
      "Variables du dataframe stores.csv:\n",
      "Nombre d'observations:  54\n",
      "Index(['store_nbr', 'city', 'state', 'type', 'cluster'], dtype='object')\n",
      "==================================================\n",
      "Variables du dataframe test.csv:\n",
      "Nombre d'observations:  3370464\n",
      "Index(['id', 'date', 'store_nbr', 'item_nbr', 'onpromotion'], dtype='object')\n",
      "3370464\n",
      "==================================================\n",
      "Variables du dataframe train.csv:\n",
      "Nombre d'observations:  125497040\n",
      "Index(['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyman\\Downloads\\Telegram Desktop\\Salesprediction\\SalesPrediction\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:199: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n",
      "c:\\Users\\tyman\\Downloads\\Telegram Desktop\\Salesprediction\\SalesPrediction\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:199: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyman\\Downloads\\Telegram Desktop\\Salesprediction\\SalesPrediction\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:199: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125497040\n",
      "==================================================\n",
      "Variables du dataframe transactions.csv:\n",
      "Nombre d'observations:  83488\n",
      "Index(['date', 'store_nbr', 'transactions'], dtype='object')\n",
      "83488\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Variables de chaque dataframe\n",
    "for file in files:\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    df = dd.read_csv(file_path, assume_missing=True)\n",
    "    print(f\"Variables du dataframe {file}:\")\n",
    "    print(\"Nombre d\\'observations: \",len(df))\n",
    "    print(df.columns)\n",
    "    if 'date' in df.columns:\n",
    "        # print(df['date'].nunique().compute())\n",
    "        print(len(df['date']))\n",
    "    print(\"=\"*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-family: Times New Roman;\">Dans un premier temps, nous fusionnons les données train avec les données relatives aux articles items et aux magasins stores. Cette fusion est essentielle pour avoir les libellés de chaque vente pour une date donnée.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les données de ventes avec les informations sur les articles et les magasins\n",
    "df = train.merge(items, on='item_nbr', how='left')\n",
    "df = df.merge(stores, on='store_nbr', how='left')\n",
    "\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données obtenues sont fusionnées avec l'ensemble des transactions et les ventes de l'essence qui impactent fortement sur les activités économiques du pays. L'objectif est de rceueillir toute information utile pour l'entrainement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les données de ventes avec les informations sur les transactions\n",
    "df = df.merge(transactions, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Fusionner les données de ventes avec les informations sur les jours fériés et les événements\n",
    "df = df.merge(holidays_events, on='date', how='left')\n",
    "\n",
    "# Fusionner les données de ventes avec les informations sur le prix du pétrole\n",
    "df = df.merge(oil, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'date', 'store_nbr', 'item_nbr', 'unit_sales', 'onpromotion',\n",
      "       'family', 'class', 'perishable', 'city', 'state', 'type_x', 'cluster',\n",
      "       'transactions', 'type_y', 'locale', 'locale_name', 'description',\n",
      "       'transferred', 'dcoilwtico'],\n",
      "      dtype='object')\n",
      "127970257\n",
      "    id        date  store_nbr  item_nbr  unit_sales  onpromotion  \\\n",
      "0  0.0  2013-01-01       25.0  103665.0         7.0          NaN   \n",
      "1  1.0  2013-01-01       25.0  105574.0         1.0          NaN   \n",
      "2  2.0  2013-01-01       25.0  105575.0         2.0          NaN   \n",
      "3  3.0  2013-01-01       25.0  108079.0         1.0          NaN   \n",
      "4  4.0  2013-01-01       25.0  108701.0         1.0          NaN   \n",
      "\n",
      "         family   class  perishable     city        state type_x  cluster  \\\n",
      "0  BREAD/BAKERY  2712.0         1.0  Salinas  Santa Elena      D      1.0   \n",
      "1     GROCERY I  1045.0         0.0  Salinas  Santa Elena      D      1.0   \n",
      "2     GROCERY I  1045.0         0.0  Salinas  Santa Elena      D      1.0   \n",
      "3     GROCERY I  1030.0         0.0  Salinas  Santa Elena      D      1.0   \n",
      "4          DELI  2644.0         1.0  Salinas  Santa Elena      D      1.0   \n",
      "\n",
      "   transactions   type_y    locale locale_name         description  \\\n",
      "0         770.0  Holiday  National     Ecuador  Primer dia del ano   \n",
      "1         770.0  Holiday  National     Ecuador  Primer dia del ano   \n",
      "2         770.0  Holiday  National     Ecuador  Primer dia del ano   \n",
      "3         770.0  Holiday  National     Ecuador  Primer dia del ano   \n",
      "4         770.0  Holiday  National     Ecuador  Primer dia del ano   \n",
      "\n",
      "  transferred  dcoilwtico  \n",
      "0       False         NaN  \n",
      "1       False         NaN  \n",
      "2       False         NaN  \n",
      "3       False         NaN  \n",
      "4       False         NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tyman\\Downloads\\Telegram Desktop\\Salesprediction\\SalesPrediction\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:199: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Afficher les premières lignes du DataFrame fusionné\n",
    "print(df.columns)\n",
    "\n",
    "# Taille du dataframe issu de la fusion\n",
    "print(len(df['date']))\n",
    "\n",
    "# Premières lignes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(1, 100, 120); font-family: Georgia;\">Echantillonnage</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"font-family: Georgia;\">Gestion des données manquantes</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable date\n",
      "0\n",
      "================================================================================\n",
      "Variable store_nbr\n"
     ]
    }
   ],
   "source": [
    "for var in list(df.columns)[1:]:\n",
    "    print(\"Variable\",var)\n",
    "    print(df[f'{var}'].isnull().sum().compute())\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Series Structure:\n",
      "npartitions=1\n",
      "city          int64\n",
      "unit_sales      ...\n",
      "Dask Name: sum, 12 expressions\n",
      "Expr=(~ NotNull(frame=Merge(7029391))).sum()\n"
     ]
    }
   ],
   "source": [
    "# Valeurs manquantes des prix du pétrole\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-family: Times New Roman;\">Les données manquantes sont supprimées pour alléger la base et nous permettre de mieux entrainer le modèle.Elle permet, en effet, de réduire la complexité de la base de données tout en éliminant les incohérences susceptibles d’affecter la performance du modèle. En éliminant ces valeurs manquantes, on allège la base, ce qui améliore non seulement la vitesse de traitement.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des valeurs manquantes\n",
    "df = df.dropna(subset=['onpromotion', 'dcoilwtico', 'transactions', 'transferred', 'locale_name', 'description', 'locale', 'type_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'observation de la base finale:  12560600\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "print(\"Nombre d'observation de la base finale: \",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ISE2 docs\\MesDocuments\\1erSemestre\\MachineLearning1\\Salesprediction\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:197: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n",
      "d:\\ISE2 docs\\MesDocuments\\1erSemestre\\MachineLearning1\\Salesprediction\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:197: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes (NA) par colonne :\n",
      "id              0\n",
      "date            0\n",
      "store_nbr       0\n",
      "item_nbr        0\n",
      "unit_sales      0\n",
      "onpromotion     0\n",
      "family          0\n",
      "class           0\n",
      "perishable      0\n",
      "city            0\n",
      "state           0\n",
      "type_x          0\n",
      "cluster         0\n",
      "transactions    0\n",
      "type_y          0\n",
      "locale          0\n",
      "locale_name     0\n",
      "description     0\n",
      "transferred     0\n",
      "dcoilwtico      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calcul du nombre de NA par colonne\n",
    "na_counts = df.isna().sum().compute()\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Nombre de valeurs manquantes (NA) par colonne :\")\n",
    "print(na_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify; font-family: Times New Roman;\">Après avoir supprimé les valeurs manquantes, nous allons exporter la base comme base de travail pour le projet de prédiction des ventes.</p>\n",
    "NB: Dans le but de réduire les données, nous ne conservons que les 15% des données les plus récentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri des données par date en ordre décroissant pour obtenir les observations les plus récentes en premier\n",
    "df = df.sort_values(by='date', ascending=False)\n",
    "\n",
    "# Echantillon (Méthode non probabilite)\n",
    "sales = df.head(int(len(df) * 0.15))\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du DataFrame train_model sous forme de fichier CSV\n",
    "sales.to_csv('..\\Data\\salesPrediction.csv', index=False)\n",
    "print(\"Le fichier CSV a été sauvegardé avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier train_model.csv en utilisant Dask\n",
    "train_model = dd.read_csv('..\\Data\\salesPrediction.csv')\n",
    "\n",
    "# Fonction pour convertir la colonne 'date' en type datetime et extraire des fonctionnalités\n",
    "def preprocess_date(df):\n",
    "    df['date'] = dd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
